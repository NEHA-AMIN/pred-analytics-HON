# Model Training Configuration for Extreme Imbalance

# Problem characteristics
problem:
  type: "binary_classification"
  positive_rate: 0.0001  # 0.01%
  primary_metric: "pr_auc"  # Precision-Recall AUC
  secondary_metrics:
    - "precision_at_1_pct"
    - "precision_at_5_pct"
    - "recall_at_1_pct"

# Baseline model
baseline:
  model_type: "logistic_regression"
  class_weight: "balanced"
  max_iter: 1000
  random_state: 42

# XGBoost configuration
xgboost:
  # Critical for extreme imbalance
  scale_pos_weight: 9999  # Ratio of negative to positive (approximation)
  
  # Model architecture
  max_depth: 6
  learning_rate: 0.05
  n_estimators: 200
  min_child_weight: 1
  
  # Regularization
  gamma: 0.1
  subsample: 0.8
  colsample_bytree: 0.8
  reg_alpha: 0.1
  reg_lambda: 1.0
  
  # Training
  objective: "binary:logistic"
  eval_metric: ["aucpr", "logloss"]
  early_stopping_rounds: 20
  
  # System
  random_state: 42
  n_jobs: -1
  tree_method: "hist"  # Faster for large datasets

# LightGBM configuration (alternative)
lightgbm:
  # Imbalance handling
  is_unbalance: true
  
  # Model architecture
  max_depth: 6
  learning_rate: 0.05
  n_estimators: 200
  num_leaves: 31
  min_child_samples: 20
  
  # Regularization
  reg_alpha: 0.1
  reg_lambda: 1.0
  subsample: 0.8
  colsample_bytree: 0.8
  
  # Training
  objective: "binary"
  metric: ["auc", "binary_logloss"]
  
  # System
  random_state: 42
  n_jobs: -1
  verbose: -1

# Hyperparameter tuning (Optuna)
tuning:
  enabled: false  # Start with defaults, tune later
  n_trials: 50
  timeout_minutes: 60
  
  # Search space
  params:
    max_depth: [4, 6, 8, 10]
    learning_rate: [0.01, 0.05, 0.1, 0.2]
    n_estimators: [100, 200, 300, 500]
    scale_pos_weight: [5000, 10000, 15000, 20000]

# Threshold tuning
threshold:
  strategy: "precision_optimized"  # Optimize for precision
  target_precision: 0.10  # Aim for 10% precision at chosen threshold
  search_range: [0.001, 0.999]
  search_steps: 100

# Feature engineering for models
features:
  # Handle categorical
  categorical_encoding: "onehot"  # For item_category
  
  # Handle missing (shouldn't have any, but safety)
  missing_strategy: "zero"
  
  # Feature selection
  drop_features:
    - "user_id"
    - "item_id" 
    - "session_id"
    - "session_end_time"
  
  target_column: "label"

# Training settings
training:
  validation_split: "temporal"  # Use val set for early stopping
  save_predictions: true
  save_model: true
  
# MLflow tracking
mlflow:
  experiment_name: "purchase_prediction"
  tracking_uri: "mlruns"
  log_params: true
  log_metrics: true
  log_model: true
  log_artifacts: true